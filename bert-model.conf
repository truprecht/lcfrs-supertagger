# bert model skips bilstm, but tunes the transformer embeddings
# lr, dropout and weight decay are best practices as described by Devlin et al.
# https://arxiv.org/pdf/1810.04805.pdf

[Training]
epochs = 5
lr = 5e-5
patience = 5
embedding = bert-base
tune_embedding = True
lstm_layers = 0
dropout = 0.1
weight_decay = 1e-2
decodertype = FfDecoder

batchsize = 32
micro_batch_size = 8

fallbackprob = 0
ktags = 10
evalfilename = disco-dop/proper.prm
only_disc = both
accuracy = both