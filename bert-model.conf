# bert model skips bilstm, but tunes the transformer embeddings
# lr, dropout and weight decay are best practices as described by Devlin et al.
# https://arxiv.org/pdf/1810.04805.pdf

[Training]
epochs = 100
lr = 5e-5

embedding = bert-base
tune_embedding = True

lstm_layers = 0

dropout = 0.1
weight_decay = 1e-2