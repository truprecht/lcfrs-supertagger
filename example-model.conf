[Training]
# max number of training epochs
epochs = 20
# learning rate
lr = 0.1

# chose one or many of 
# * word -- for one-hot word embeddings tuned during training
# * char -- for character-bilstm word embeddings tuned during training
# * flair -- for (bidirectional) flair word embeddings matching the corpus language
# * fasttext -- for fasttext word embeddings matching the corpus' language
# * pos -- for pos embeddings tuned during training
# * bert-base -- for the top 4 layers of a bert model that matches the copus' language,
#                other models supplied by huggingface may also work,
#                see https://huggingface.co/transformers/pretrained_models.html
embedding = word char pos
# if pos is in embedding
pos_embedding_dim = 20
# if word is in embedding:
# least amount of occurrences for a word to be adopted in the vocabulary
word_embedding_dim = 10
word_minfreq = 3
# if char is in embedding
char_embedding_dim = 10
char_bilstm_dim = 10
# if True, fine-tunes bert-base and flair embeddings
# word and char embeddings are trained either way
tune_embedding = False

# bilstm layer on top of embeddings, setting this to 0 skips the bilstm
lstm_layers = 1
# hidden size of the bilstm, its output is twice the size
lstm_size = 100

# regulization
weight_decay = 0.01
# usual dropout
dropout = 0
# variational dropout, i.e. drops the same dimensions in each sequence position
locked_dropout = 0.1
# drops entire sequence positions
word_dropout = 0.05