[Training]
# optimizer class in torch.optim
optimizer = Adam
# max number of training epochs
epochs = 16
# learning rate
lr = 1e-3
# the scores of the dev set are monitored;
# if there is no increase in `patience` epochs, then the lr is multiplied by
# `lr_decay`, if it falls below `min_lr`, then the training is cancelled
patience = 2
min_lr = 1e-5
lr_decay = 0.5

# chose one or many of 
# * word -- for one-hot word embeddings tuned during training
# * char -- for character-bilstm word embeddings tuned during training
# * flair -- for (bidirectional) flair word embeddings matching the corpus language
# * fasttext -- for fasttext word embeddings matching the corpus language
# * pos -- for pos embeddings tuned during training
# * bert-base -- for the top 4 layers of a bert model that matches the copus language,
#                other models supplied by huggingface may also work,
#                see https://huggingface.co/transformers/pretrained_models.html
embedding = word char
# if word is in embedding:
# least amount of occurrences for a word to be adopted in the vocabulary
word_embedding_dim = 64
word_minfreq = 3
# if char is in embedding
char_embedding_dim = 16
char_bilstm_dim = 16
# if True, fine-tunes bert-base and flair embeddings
# word and char embeddings are trained either way
tune_embedding = False

# bilstm layer on top of embeddings, setting this to 0 skips the bilstm
lstm_layers = 1
# hidden size of the bilstm, its output is twice the size
lstm_size = 256

# regulization
weight_decay = 0
# usual dropout
dropout = 0
# variational dropout, i.e. drops the same dimensions in each sequence position
locked_dropout = 0
# drops entire sentence positions
word_dropout = 0
# dropout between bilstm layers,
# if unset, defaults to `dropout`
lstm_dropout = 0

decodertype = LmDecoder
decoder_hidden_dim = 0
decoder_embedding_dim = 0
sample_gold_tags = 0.9


batchsize = 32
micro_batch_size = 8

fallbackprob = 0
ktags = 10
evalfilename = disco-dop/proper.prm
only_disc = both
accuracy = both