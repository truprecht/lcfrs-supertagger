[Training]
# optimizer class in torch.optim
optimizer = Adam
# max number of training epochs
epochs = 20
# learning rate
lr = 0.1
# the scores of the dev set are monitored;
# if there is no increase in `patience` epochs, then the lr is multiplied by
# `lr_decay`, if it falls below `min_lr`, then the training is cancelled
patience =20
min_lr = 1e-6
lr_decay = 0.75

# chose one or many of 
# * word -- for one-hot word embeddings tuned during training
# * char -- for character-bilstm word embeddings tuned during training
# * flair -- for (bidirectional) flair word embeddings matching the corpus language
# * fasttext -- for fasttext word embeddings matching the corpus language
# * pos -- for pos embeddings tuned during training
# * bert-base -- for the top 4 layers of a bert model that matches the copus language,
#                other models supplied by huggingface may also work,
#                see https://huggingface.co/transformers/pretrained_models.html
embedding = word char pos
# if pos is in embedding
pos_embedding_dim = 20
# if word is in embedding:
# least amount of occurrences for a word to be adopted in the vocabulary
word_embedding_dim = 10
word_minfreq = 1
# if char is in embedding
char_embedding_dim = 10
char_bilstm_dim = 10
# if True, fine-tunes bert-base and flair embeddings
# word and char embeddings are trained either way
tune_embedding = False

# bilstm layer on top of embeddings, setting this to 0 skips the bilstm
lstm_layers = 0
# hidden size of the bilstm, its output is twice the size
lstm_size = 30

# regulization
weight_decay = 0
# usual dropout
dropout = 0
# variational dropout, i.e. drops the same dimensions in each sequence position
locked_dropout = 0
# drops entire sentence positions
word_dropout = 0
# dropout between bilstm layers,
# if unset, defaults to `dropout`
lstm_dropout = 0

# decoder type, one of:
# * FfDecoder -- just a feed forward layer
# * LmDecoder -- the output is fed into an LSTM language model (Vasvani 2016)
decodertype = FfDecoder
# decoder_hidden_dim = 128
# decoder_embedding_dim = 64
# sample_gold_tags = 0.9

fallbackprob = 0
# determines how many supertags per sentence position are used
ktags = 10
# configuration file for evalb-style F-scores
evalfilename = disco-dop/proper.prm
# batch size for neural network predictions
batchsize = 32
micro_batch_size = 10
# reports parsing scores for all constituents (False), discontinuous constituents (True),
# both separately (both), or as specified in ```evalfilename``` (param)
only_disc = both
# reports supertag prediction accuracy among k best tags (kbest),
# for the best tag (best), both separately (both), or none at all (none)
# accuracy = both
# reports accuracy of the predicted tags defined in separate_attribs
othertag_accuracy = True
accuracy = none